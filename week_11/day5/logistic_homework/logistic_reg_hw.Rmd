---
title: "Logistic regression homework"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    css: ../../../styles.css
  pdf_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align = "center")
```

# MVP

You have been provided with a set of data on customer purchases of either 'Citrus Hill' (`purchase = 'CH'`) or 'Minute Maid' (`purchase = 'MM'`) orange juice, together with some further attributes of both the customer and the store of purchase. A data dictionary is also provided in the `data` directory.

```{r}
library(tidyverse)
library(janitor)
library(modelr)
library(broom)
library(GGally)
library(rJava)
library(glmulti)
```


We would like you to build the best **predictive classifier** you can of whether a customer is likely to buy Citrus Hill or Minute Maid juice. Use **logistic regression** to do this. You should use either train-test splitting or cross-validation to evaluate your classifier. The metric for 'best classifier' will be **highest AUC value** either in the test set (for train-test splitting) or from cross-validation.

import and clean the data:
```{r}
juice <- clean_names(read_csv("data/orange_juice.csv"))
unique(juice$store7)
# turn data type character into factor
oj <- juice %>% 
  rename(
    week_of_purchase = weekof_purchase,
    store_7 = store7
  ) %>%
  mutate(purchase = factor(purchase),
         store_id = factor(store_id),
         special_ch = as.logical(special_ch),
         special_mm = as.logical(special_mm),
         store_7 = store_7 == "Yes",
         store = as_factor(store))
```


**Issues we faced, thoughts we had**

* This is quite a tough, open-ended exercise. We decided early on to use an automated approach to model selection using `glmulti()`, but feel free to use a manual approach if you prefer!
* The `Purchase` dependent variable will require wrangling to work in logistic regression. We replaced it with a `purchase_mm` logical variable.
* Wrangle other categorical variables to be factors too.
* `WeekOfPurchase` is also quite tough to deal with: should it be added as a factor variable (it will lead to many coefficients), left as numeric, or omitted entirely? See if you can come up with a strategy to decide what to do with it.
* Check for aliased variables and remove any aliases before you set off to find your best models. Remember, you can use something like `alias(purchase_mm ~ ., data = oj)` to do this, the dot `.` here means 'all variables'. Aliased variables will be listed down the left-hand side, and you can subsequently remove them.

**EDA**

Let’s see the split of the data by purchase.

```{r}
oj %>%
  group_by(purchase) %>%
  summarise(n = n())
```

We also check the number of distinct values of week_of_purchase and how many data points we have for each week:

```{r}
oj %>%
  group_by(week_of_purchase) %>%
  summarise(n = n())
```
```{r}
oj <- oj %>%
  mutate(week_of_purchase_fac = as_factor(week_of_purchase))

```

For logistic regression, we mutate the dependent variable to be logical, we’ll use purchase_mm as level “MM” is the minority, but this is really an arbitrary choice:

```{r}
oj <- oj %>%
  mutate(purchase_mm = purchase == "MM") %>%
  select(-purchase)
```

Let’s check for aliases in the independent variables:
```{r}
alias(purchase_mm ~ ., data = oj)
```


So we find sale_price_mm, sale_price_ch, price_diff, store_7, list_price_diff and store can be derived from other variables, so we’ll remove them. As expected, we also see an alias between week_of_purchase and week_of_purchase_fac, so, again, we should be careful to include only one of these variable in any given model.

```{r}
oj_trim <- oj %>%
  select(-c("sale_price_mm", "sale_price_ch", "price_diff", "store_7", "list_price_diff", "store"))
```

Let’s split the variables and look at pairs plots to investigate the relationships of variables with purchase_mm. Note we leave out week_of_purchase_fac here as it has too many levels - we will plot it separately below.

```{r}
names(oj_trim)
```

```{r}
set1 <- oj_trim %>%
  select(week_of_purchase, store_id, price_ch, price_mm, disc_ch, disc_mm, purchase_mm)

set2 <- oj_trim %>%
  select(special_ch, special_mm, loyal_ch, pct_disc_mm, pct_disc_ch, purchase_mm)

```


```{r}
ggpairs(set1)
```

```{r}
ggsave("pairs_plot_set1.png", width = 10, height = 10, units = "in")
```

```{r}
ggpairs(set2)
```
```{r}
ggsave("pairs_plot_set2.png", width = 10, height = 10, units = "in")
```
  
Now look at how purchase_mm varies with week_of_purchase_fac
```{r}
oj_trim %>%
  ggplot(aes(x = week_of_purchase_fac, fill = purchase_mm)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```


Build the test-train sets:
```{r}
test_indices <- sample(1:nrow(oj_trim), size = as.integer(nrow(oj_trim) * 0.2))

train <- oj_trim %>%
  slice(-test_indices)

test <- oj_trim %>%
  slice(test_indices)

# sanity check
nrow(train) + nrow(test) == nrow(oj_trim)
```

Let’s check the distribution of outcome in the training and testing sets to see that they are roughly comparable:
```{r}
train %>%
  tabyl(purchase_mm)
```

```{r}
test %>%
  tabyl(purchase_mm)
```


**`glmulti()` hints**

If you decide to use `glmulti()` be prepared for your `R` session to hang if you decide to abort a run! The reason for this is that `glmulti()` actually uses a separate Java runtime to do its thing in the background, and unfortunately `R` can't instruct Java to terminate on request. D'oh! Accordingly, make sure you **save any changes** to your work **before** each `glmulti()` run. That way, you can force quit `RStudio` if necessary without losing work. 

Here are some example inputs for using `glmulti()` with logistic regression for a variety of purposes.

* Run an exhaustive search (i.e. all possible models) over all 'main effects only' logistic regression models using BIC as the quality metric

```{r, eval=FALSE}
glmulti_search_all_mains <- glmulti(
  purchase_mm ~ . - week_of_purchase_fac, 
  data = train,
  level = 1,               # No interactions considered, main effects only
  method = "h",            # Exhaustive approach
  crit = "bic",            # BIC as criteria
  confsetsize = 10,        # Keep 10 best models
  plotty = F, 
  report = T,              # No plots, but provide interim reports
  fitfunction = "glm",     # glm function
  family = binomial(link = "logit")) # binomial family for logistic regression

summary(glmulti_search_all_mains)
```

* Imagine now you've found the main effects model with lowest BIC, and you would like to add on a single pair interaction considering only main effects in the model. Which single pair addition leads to lowest BIC?

```{r, eval=FALSE}
glmulti_search_previous_mains_one_pair <- glmulti(
  purchase_mm ~ var_a + var_b + var_c + var_d + var_e, 
  data = train,
  level = 2,               # Interactions considered
  method = "h",            # Exhaustive approach
  crit = "bic",            # BIC as criteria
  confsetsize = 10,        # Keep 10 best models
  marginality = TRUE,      # consider pairs only if both main effects in model
  minsize = 6,             # minsize, maxsize and marginality here force 
  maxsize = 6,             # inclusion of a single pair beyond the five main effects
  plotty = F, 
  report = T,              # No plots, but provide interim reports
  fitfunction = "glm",     # glm function
  family = binomial(link = "logit")) # binomial family for logistic regression

summary(glmulti_search_previous_mains_one_pair)
```

* In cases where an exhaustive search isn't possible because there are too many possible models to search through, you could try a search using a genetic algorithm. Here, run a genetic algorithm search over all main effects plus pair models, using lowest AIC as the quality criterion

```{r, eval=FALSE}
glmulti_ga_search_with_pairs_aic <- glmulti(
  purchase_mm ~ .,
  data = train,
  level = 2,               # Interactions considered
  method = "g",            # Genetic algorithm approach
  crit = "aic",            # AIC as criteria
  confsetsize = 10,        # Keep 10 best models
  marginality = TRUE,      # consider pairs only if both main effects in model
  plotty = F, 
  report = T,              # No plots, but provide interim reports
  fitfunction = "glm",     # glm function
  family = binomial(link = "logit")) # binomial family for logistic regression

summary(glmulti_ga_search_with_pairs_aic)
```

